{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "block_size = 64\n",
    "batch_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_heads = 6\n",
    "n_layers = 6\n",
    "head_size = n_embd / n_heads\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch shakespear data from internet\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# Download the file from the URL if it does not exist\n",
    "shakespeare_path = \"shakespeare.txt\"\n",
    "if not os.path.isfile(shakespeare_path):\n",
    "    urllib.request.urlretrieve(shakespeare_url, shakespeare_path)\n",
    "\n",
    "with open(shakespeare_path) as f:\n",
    "    shakespeare_text = f.read()\n",
    "print(shakespeare_text[:148])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 65\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique characters in the text\n",
    "chars = sorted(list(set(shakespeare_text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Number of unique characters: {vocab_size}\")\n",
    "\n",
    "# Create a mapping from characters to indices and vice versa\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Convert the text to a tensor\n",
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda i: \"\".join([itos[ch] for ch in i])\n",
    "\n",
    "data = encode(shakespeare_text)\n",
    "data = torch.tensor(data, dtype = torch.long)\n",
    "\n",
    "train_data = data[:int(0.9 * len(data))]\n",
    "val_data = data[int(0.9 * len(data)):]\n",
    "\n",
    "# Test the function\n",
    "print(decode(data[:148].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 64]) torch.Size([256, 64])\n",
      " some more mightier member\n",
      "That sets them on: let me have way, m\n",
      "some more mightier member\n",
      "That sets them on: let me have way, my\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    idx = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    inputs = torch.stack([data[i:i+block_size] for i in idx])\n",
    "    outputs = torch.stack([data[i+1:i+block_size+1] for i in idx])\n",
    "    x, y = inputs.to(device), outputs.to(device)\n",
    "    return inputs, outputs\n",
    "\n",
    "# Test the function\n",
    "inputs, outputs = get_batch(\"train\")\n",
    "print(inputs.shape, outputs.shape)\n",
    "\n",
    "# print the first example\n",
    "print(decode(inputs[0].tolist()))\n",
    "print(decode(outputs[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"\n",
    "    Estimate the loss of the model on the training and validation sets\n",
    "    \"\"\"\n",
    "    out = {}    # store the loss values for the training and validation sets\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-attention:\n",
    "Each token emits three vectors:\n",
    "* Query: represents what the token is looking for\n",
    "* Key: represents what the token contains\n",
    "* value: represents the values to be averaged over\n",
    "\n",
    "For each node/token we get a similarity measure between the query and all the other keys using a dot product. This similarity measure determines how much attention is paid to a given token at time t, and we aggregate the values based on the similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* Decoding: only care about the past\n",
    "* Encoding: include the future as well (no masking)\n",
    "* attention is just a communication protocol between a set of nodes. Its like learning a probabilistic graph where the probability determines how much attention/communication to derive from another node.\n",
    "* positional encoding is necessary else the nodes are just randomly placed in space\n",
    "* Self-attention: because the keys and values come from x (the data itself)\n",
    "* Cross-attention: if keys and values are supplied externally\n",
    "* Dividing by sqrt head size to preserve variance at initialization. Else the exponential may be too peaky (favoring few values). We want diffuse weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, C, head_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.key = nn.Linear(C, head_size, bias=False)\n",
    "        self.query = nn.Linear(C, head_size, bias=False)\n",
    "        self.value = nn.Linear(C, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size))) # not a parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of dim (B,T,C)\n",
    "        B, T, C= x.shape\n",
    "\n",
    "        k = self.key(x)     # (B, T, head_size)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) / np.sqrt(k.shape[-1])\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        out = wei @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, C, head_size, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList([Head(C, head_size) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(n_heads * head_size, C)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.linear(out)\n",
    "        out = self.dropout(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Block, self).__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.sa_heads = MultiHeadAttention(n_embd, head_size, n_heads)\n",
    "        self.ff = FeedForwardBlock()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 64, 65]) tensor(4.3140, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# What is a bigram language model\n",
    "# A bigram language model is a model that predicts the next character given the current character.\n",
    "# The model is trained on a sequence of characters and learns the probability distribution of the next character given the current character.\n",
    "# The model can be used to generate text by sampling the next character from the probability distribution.\n",
    "\n",
    "class BigramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = torch.nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential( *[Block() for _ in range(n_layers)] )\n",
    "        self.ln = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "    def forward(self, x, target = None):\n",
    "        # x is a tensor of shape (batch_size, sequence_length)\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_embds = self.embedding(x)   # (B,T) -> (B,T,C)\n",
    "        pos_embds = self.position_embedding(torch.arange(T).to(x.device)) # (T,C)\n",
    "        x = tok_embds + pos_embds # (B,T,C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        # target is a tensor of shape (batch_size, sequence_length)\n",
    "        if target is not None:\n",
    "            # Calculate the loss\n",
    "            # logits -> (characters in sequence, logits)\n",
    "            # targets -> (outputs/classes in sequence)\n",
    "            # each target value is the idx of the output and is compared to the logit channels\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, vocab_size), target.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, x, length = 1):\n",
    "        # x is a tensor of shape (batch_size, sequence_length)\n",
    "        # The embedding layer converts the input tensor to a tensor of shape (batch_size, sequence_length, vocab_size)\n",
    "        logits, loss = self.forward(x[:, -block_size:])\n",
    "\n",
    "        # Generate the next character\n",
    "        next_char = torch.multinomial(torch.nn.functional.softmax(logits[:, -1], dim=-1), 1)\n",
    "\n",
    "        # Append the next character to the input tensor\n",
    "        x = torch.cat([x, next_char], dim=1)\n",
    "\n",
    "        # Repeat the process for the specified length\n",
    "        for i in range(length - 1):\n",
    "            logits, loss = self.forward(x[:, -block_size:])\n",
    "            next_char = torch.multinomial(torch.nn.functional.softmax(logits[:, -1], dim=-1), 1)\n",
    "\n",
    "            x = torch.cat([x, next_char], dim=1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Create a bigram language model\n",
    "model = BigramLanguageModel().to(device)\n",
    "\n",
    "# Test the model\n",
    "inputs, outputs = get_batch(\"train\")  # x: (B,T), y: (B,T)\n",
    "logits, loss = model(inputs, outputs) # logits: (B,T,C), loss: scalar\n",
    "print(logits.shape, loss)\n",
    "\n",
    "# generate text\n",
    "# inputs, outputs = get_batch(\"train\")\n",
    "# generated = model.generate(inputs, 100)\n",
    "# print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    X, Y = get_batch(\"train\")\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(X, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Iter: {iter}, Train loss: {losses['train']}, Val loss: {losses['val']}\")\n",
    "\n",
    "# Generate text\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Notes:\n",
    "\n",
    "* Dropout:\n",
    "    * Added before residual connection\n",
    "    * Added after projection in multihead\n",
    "    * Can be added on the weights in the heads before multiplication\n",
    "* LayerNorm:\n",
    "    * Normalized on an embedding level. Has parameters so that it can scale back up if necessary (depending on optimization)\n",
    "* BatchNorm:\n",
    "    * Normalizes over examples in a batch\n",
    "* Reduce learning_rate for deeper networks\n",
    "\n",
    "\n",
    "### Notes on Architecture:\n",
    "\n",
    "* Decoder only architecture (has tril weights)\n",
    "* The Encoder in the original paper was for language translation:\n",
    "    * You encode the original sentence and condition on it\n",
    "    * full weights\n",
    "    * Feed keys and values from the encoder to the decoder\n",
    "* Training Stages: \n",
    "    1) Pretraining: decoding only\n",
    "    2) Finetuning / Alignment: through RLHF (Labelers label or rank outputs and we use RL to maximize reward as per the feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate tests and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# for a function of dim (B, T, C), create a function that takes the average of the prior characters for each character at time t\n",
    "def xbow(x):\n",
    "    \"\"\"\n",
    "    Compute the average context for each character in the input tensor\n",
    "    \"\"\"\n",
    "    # x is a tensor of shape (batch_size, sequence_length)\n",
    "    # Create a tensor of shape (batch_size, sequence_length) to store the average context\n",
    "    avg = torch.zeros_like(x, dtype=torch.float)\n",
    "\n",
    "    for b in range(x.size(0)):\n",
    "        for t in range(x.size(1)):\n",
    "            # Compute the average context for each character at time t\n",
    "            avg[b, t] = x[b, :t+1].mean(dim=0)\n",
    "    return avg\n",
    "\n",
    "print(xbow(x)[1])\n",
    "\n",
    "def xbow_efficient(x):\n",
    "    \"\"\"\n",
    "    Compute the average context for each character in the input tensor\n",
    "    \"\"\"\n",
    "    # T = x.shape[1]\n",
    "    # wei = torch.tril(torch.ones(T,T))\n",
    "    # wei = wei / wei.sum(1, keepdim=True)\n",
    "    # avg = wei @ x \n",
    "    # return avg\n",
    "    avg_mat = torch.tril(torch.ones(T,T, dtype=torch.float, device=x.device)) \\\n",
    "                / torch.arange(1, T+1, device=x.device, dtype=torch.float).unsqueeze(1)\n",
    "    avg = torch.matmul(avg_mat, x.float())\n",
    "    return avg\n",
    "\n",
    "def xbow_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the average context for each character in the input tensor\n",
    "    \"\"\"\n",
    "    T = x.shape[1]\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    wei = torch.zeros((T,T))\n",
    "    wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "    wei = torch.nn.functional.softmax(wei, dim=1)\n",
    "    avg = wei @ x \n",
    "    return avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn((B, T, C))\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)      #(B, T, head_size)\n",
    "q = query(x)    #(B, T, head_size)\n",
    "v = value(x)    #(B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) / np.sqrt(head_size) # (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei, dim=-1)\n",
    "\n",
    "avg = wei @ v\n",
    "\n",
    "avg.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
